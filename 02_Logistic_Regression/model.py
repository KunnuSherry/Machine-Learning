# -*- coding: utf-8 -*-
"""HeartAttackEDAandFeatureEngineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aS8tNtw3lzJBB4vKiiBlqE6Z62QXSYs

**Importing Libraries for EDA and Feature Engineering**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns



#Loading dataset
import pandas as pd

with open("heart_attack_dataset.csv", "r") as f:
    print("File can be opened!")

df = pd.read_csv("heart_attack_dataset.csv")


df.columns

df.info()

"""We Can observe we have all not null values so we can move further

"""

df_object_columns = df.select_dtypes(include='object').columns
df_object_columns

df_without_object = pd.get_dummies(df, drop_first=True).astype(int)
df_without_object.head()

df_without_object.info()

import pandas as pd

# Compute correlation matrix
correlation_matrix = df_without_object.corr()

# Select features most correlated with Outcome_Heart Attack
correlation_with_outcome = correlation_matrix['Outcome_No Heart Attack'].abs().sort_values(ascending=False)

# Select top features (excluding Outcome_Heart Attack itself)
top_features = correlation_with_outcome.index[1:10]  # Get top 9 correlated features

# Filter dataset
df_selected = df_without_object[top_features]
output_feature = df_without_object['Outcome_No Heart Attack']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_selected, output_feature, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)
# Accuracy 50percent

# from xgboost import XGBClassifier

# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
# xgb_model.fit(X_train, y_train)

# y_pred_xgb = xgb_model.predict(X_test)
# accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
# print("XGBoost Accuracy:", accuracy_xgb)

# from sklearn.ensemble import RandomForestClassifier

# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
# rf_model.fit(X_train, y_train)

# y_pred_rf = rf_model.predict(X_test)
# accuracy_rf = accuracy_score(y_test, y_pred_rf)
# print("Random Forest Accuracy:", accuracy_rf)

top_features

for feature in top_features:
    print(f"{feature}: {df_without_object[feature].unique()}")

import pickle
pickle.dump(model, open("iri.pkl", 'wb'))

"""Model is trained but it is not highly accurate. due to the dataset. But we made it for just training purpose"""